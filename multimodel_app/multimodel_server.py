import argparse
import asyncio
import json
import os

import cv2
import numpy as np
import torch
import onnxruntime as ort
from ultralytics import YOLO

from aiohttp import web
from aiortc import RTCPeerConnection, RTCSessionDescription, VideoStreamTrack
from av import VideoFrame
from concurrent.futures import ThreadPoolExecutor

pcs = set()
global_data_channel = None  # ì„œë²„ ì¸¡ ë°ì´í„° ì±„ë„ ì „ì—­ ë³€ìˆ˜

# âœ… CORS ë¯¸ë“¤ì›¨ì–´ ì¶”ê°€
@web.middleware
async def cors_middleware(request, handler):
    if request.method == 'OPTIONS':  # Preflight ìš”ì²­ ì²˜ë¦¬
        return web.Response(headers={
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Methods": "POST, GET, OPTIONS",
            "Access-Control-Allow-Headers": "Content-Type"
        })
    
    response = await handler(request)
    response.headers["Access-Control-Allow-Origin"] = "*"  # ëª¨ë“  ë„ë©”ì¸ì—ì„œ ì ‘ê·¼ í—ˆìš©
    return response

# ë‘ ê°œì˜ YOLO ëª¨ë¸ ë¡œë“œ
try:
    model_path_1 = "/Users/taeheon/driver_monitoring/weights/yolov11n_20250226_01_41_20_e50b32_dataset_calling_drinking_only/weights/best.pt"
    model_path_2 = "/Users/taeheon/driver_monitoring/weights/yolov11n_20250226_01_41_20_e50b32_dataset_calling_drinking_only/weights/best.pt"
    model_1 = YOLO(model_path_1)
    model_2 = YOLO(model_path_2)
    # model_1 = ort.InferenceSession("/Users/taeheon/driver_monitoring/weights/yolov11n_20250226_01_41_20_e50b32_dataset_calling_drinking_only/weights/best.onnx")
    # model_2 = ort.InferenceSession("/Users/taeheon/driver_monitoring/weights/yolov11n_20250226_075134_e50b32_dataset_face_class_only/weights/best.onnx")
    print("âœ… ë‘ ê°œì˜ YOLO ëª¨ë¸ ë¡œë“œ ì„±ê³µ!", flush=True)
except Exception as e:
    print("âŒ ëª¨ë¸ ë¡œë“œ ì¤‘ ì—ëŸ¬ ë°œìƒ:", e)
    model_1, model_2 = None, None

# âœ… ëª¨ë¸ ì›œì—…: dummy ì´ë¯¸ì§€ë¥¼ ì´ìš©í•´ ì˜ˆë¹„ ì¸í¼ëŸ°ìŠ¤ ìˆ˜í–‰
if model_1 and model_2:
    # dummy_img = np.ones((1, 3, 320, 320), dtype=np.float32)  # ONNX ì…ë ¥ í˜•ì‹
    dummy_img = np.ones((480, 640, 3), dtype=np.uint8) * 255  # pt ì…ë ¥ í˜•ì‹
    try:
        _ = model_1.predict(dummy_img, verbose=False)
        _ = model_2.predict(dummy_img, verbose=False)
        # _ = model_1.run(None, {'images' : dummy_img})
        # _ = model_2.run(None, {'images' : dummy_img})
        print("âœ… ëª¨ë¸ ì›œì—… ì™„ë£Œ!", flush=True)
    except Exception as e:
        print("âŒ ëª¨ë¸ ì›œì—… ì¤‘ ì—ëŸ¬:", e)

class VideoTransformTrack(VideoStreamTrack):
    def __init__(self, track, data_channel=None, inference_interval=10):
        super().__init__()
        self.track = track
        self.data_channel = data_channel
        self.inference_interval = inference_interval
        self.frame_count = 0
        self.executor = ThreadPoolExecutor(max_workers=2)

    async def run_inference(self, rgb_img):
        loop = asyncio.get_event_loop()

        # ONNX ëª¨ë¸ ì…ë ¥ ì „ì²˜ë¦¬ (ì •ê·œí™” ë° ì°¨ì› ë³€í™˜)
        img_resized = cv2.resize(rgb_img, (640, 640))  # ONNX ì…ë ¥ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
        img_float = img_resized.astype(np.float32) / 255.0  # ì •ê·œí™”
        img_onnx = np.expand_dims(img_float.transpose(2, 0, 1), axis=0)  # (H, W, C) â†’ (1, C, H, W)

        results_1, results_2 = await asyncio.gather(
            loop.run_in_executor(self.executor, lambda: model_1.predict(rgb_img, verbose=False)),
            loop.run_in_executor(self.executor, lambda: model_2.predict(rgb_img, verbose=False))
            # loop.run_in_executor(self.executor, lambda: model_1.run(None, {'images' : img_onnx})),
            # loop.run_in_executor(self.executor, lambda: model_2.run(None, {'images' : img_onnx}))
        )
        print("results_1 shape : ", results_1.shape, "results_2 shape : ", results_2.shape)
        return results_1[0], results_2[0]

    async def recv(self):
        frame = await self.track.recv()
        img = frame.to_ndarray(format="bgr24")
        self.frame_count += 1

        if self.frame_count % self.inference_interval == 0:
            rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            try:
                results_1, results_2 = await self.run_inference(rgb_img)

                def extract_detection_data(results):
                    if results.boxes is not None and len(results.boxes) > 0:
                        data = results.boxes.data.cpu().numpy()
                        highest_idx = int(np.argmax(data[:, 4]))
                        highest_confidence = data[highest_idx, 4]
                        highest_label = results.names[int(data[highest_idx, 5])]
                        return highest_label, highest_confidence
                    return "None", 0.0

                highest_label_1, highest_confidence_1 = extract_detection_data(results_1)
                highest_label_2, highest_confidence_2 = extract_detection_data(results_2)

                print(f"âœ… Model 1: {highest_label_1} ({highest_confidence_1:.2f})")
                print(f"âœ… Model 2: {highest_label_2} ({highest_confidence_2:.2f})")

            except Exception as e:
                print("âŒ YOLO ì¸í¼ëŸ°ìŠ¤ ì¤‘ ì—ëŸ¬ ë°œìƒ:", e)
                highest_label_1, highest_confidence_1 = "None", 0.0
                highest_label_2, highest_confidence_2 = "None", 0.0

            global global_data_channel
            if self.data_channel is None and global_data_channel is not None:
                self.data_channel = global_data_channel

            if self.data_channel and self.data_channel.readyState == "open":
                try:
                    self.data_channel.send(json.dumps({
                        "model_1_label": highest_label_1,
                        "model_1_confidence": float(highest_confidence_1),
                        "model_2_label": highest_label_2,
                        "model_2_confidence": float(highest_confidence_2)
                    }))
                except Exception as e:
                    print("âŒ ë°ì´í„° ì±„ë„ ì „ì†¡ ì—ëŸ¬:", e)

        new_frame = VideoFrame.from_ndarray(img, format="bgr24")
        new_frame.pts = frame.pts
        new_frame.time_base = frame.time_base
        return new_frame

async def offer(request):
    params = await request.json()
    offer = RTCSessionDescription(sdp=params["sdp"], type=params["type"])

    pc = RTCPeerConnection()
    pcs.add(pc)
    print("âœ… PeerConnection ìƒì„±ë¨")

    global global_data_channel
    data_channel = pc.createDataChannel("detection")
    global_data_channel = data_channel
    print("âœ… ì„œë²„ ë°ì´í„° ì±„ë„ ìƒì„± ì™„ë£Œ!")

    @pc.on("track")
    def on_track(track):
        print("ğŸ¥ ìˆ˜ì‹ ëœ íŠ¸ë™:", track.kind)
        if track.kind == "video":
            local_video = VideoTransformTrack(track, global_data_channel)
            pc.addTrack(local_video)

    await pc.setRemoteDescription(offer)
    answer = await pc.createAnswer()
    await pc.setLocalDescription(answer)

    response = web.Response(
        content_type="application/json",
        text=json.dumps({
            "sdp": pc.localDescription.sdp,
            "type": pc.localDescription.type
        }),
    )
    response.headers["Access-Control-Allow-Origin"] = "*"
    return response

# âœ… ì„œë²„ ì¢…ë£Œ ì‹œ PeerConnection ì •ë¦¬
async def on_shutdown(app):
    coros = [pc.close() for pc in pcs]
    await asyncio.gather(*coros)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="WebRTC YOLO ê°ì²´ íƒì§€ ì„œë²„ (ë©€í‹° ëª¨ë¸)")
    parser.add_argument("--host", default="0.0.0.0", help="ì„œë²„ í˜¸ìŠ¤íŠ¸")
    parser.add_argument("--port", type=int, default=8080, help="ì„œë²„ í¬íŠ¸")
    args = parser.parse_args()

    app = web.Application(middlewares=[cors_middleware])  # âœ… CORS ë¯¸ë“¤ì›¨ì–´ ì¶”ê°€
    app.on_shutdown.append(on_shutdown)  # âœ… ì„œë²„ ì¢…ë£Œ ì‹œ PeerConnection ì •ë¦¬
    app.router.add_post("/offer", offer)

    print("ğŸš€ ë©€í‹° ëª¨ë¸ YOLO ì„œë²„ ì‹œì‘...")
    web.run_app(app, host=args.host, port=args.port)
